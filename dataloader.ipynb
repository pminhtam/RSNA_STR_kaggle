{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import albumentations as albu\n",
    "from skimage.color import gray2rgb\n",
    "import functools\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = '/hdd/tam/kaggle/rsna/train.csv'\n",
    "jpeg_dir = '/hdd/tam/kaggle/rsna/train-jpegs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv_path)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation(y=256,x=256):\n",
    "    train_transform = [albu.RandomBrightnessContrast(p=0.3),\n",
    "                           albu.VerticalFlip(p=0.5),\n",
    "                           albu.HorizontalFlip(p=0.5),\n",
    "                           albu.Downscale(p=1.0,scale_min=0.35,scale_max=0.75,),\n",
    "                           albu.Resize(y, x)]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "formatted_settings = {\n",
    "            'input_size': [3, 224, 224],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225],}\n",
    "def preprocess_input(\n",
    "    x, mean=None, std=None, input_space=\"RGB\", input_range=None, **kwargs\n",
    "):\n",
    "\n",
    "    if input_space == \"BGR\":\n",
    "        x = x[..., ::-1].copy()\n",
    "\n",
    "    if input_range is not None:\n",
    "        if x.max() > 1 and input_range[1] == 1:\n",
    "            x = x / 255.0\n",
    "\n",
    "    if mean is not None:\n",
    "        mean = np.array(mean)\n",
    "        x = x - mean\n",
    "\n",
    "    if std is not None:\n",
    "        std = np.array(std)\n",
    "        x = x / std\n",
    "\n",
    "    return x\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "\n",
    "def get_validation_augmentation(y=256,x=256):\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [albu.Resize(y, x)]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    \"\"\"\n",
    "    Convert image or mask.\n",
    "    \"\"\"\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "  \n",
    "\n",
    "def norm(img):\n",
    "    img-=img.min()\n",
    "    return img/img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset2D(Dataset):\n",
    "    def __init__(self,df,transforms = None,preprocessing=None,size=256,mode='val'):\n",
    "        df = df[['StudyInstanceUID','SeriesInstanceUID','SOPInstanceUID','pe_present_on_image','negative_exam_for_pe','rightsided_pe','leftsided_pe','central_pe','rv_lv_ratio_gte_1','rv_lv_ratio_lt_1','chronic_pe','acute_and_chronic_pe','indeterminate']]\n",
    "        self.df_ori = df\n",
    "        self.df_main = df.values\n",
    "        if mode=='val':\n",
    "            self.df = self.df_main\n",
    "        else:\n",
    "            self.update_train_df()\n",
    "#         self.df_group = self.df.groupby(['StudyInstanceUID','SeriesInstanceUID']).agg(list)  \n",
    "        self.df_label_img = self.df_ori.groupby(['StudyInstanceUID','SeriesInstanceUID','SOPInstanceUID'])['pe_present_on_image'].apply(list)\n",
    "        self.transforms = transforms\n",
    "        self.preprocessing = preprocessing\n",
    "        self.size=size\n",
    "        self.pool = mp.pool.ThreadPool(16)\n",
    "    \n",
    "    def process_one_image(self,image_path):\n",
    "        img = cv2.imread(image_path)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        if self.preprocessing:\n",
    "            img = self.preprocessing(image=img)['image']\n",
    "#         imgs.append(img)\n",
    "        name_split = image_path.split(\"/\")\n",
    "        row_0 = name_split[-3]\n",
    "        row_1 = name_split[-2]\n",
    "        row_2 = name_split[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "#         label_imgs.append(self.df_label_img[row_0][row_1][row_2][0])\n",
    "#         return img, self.df_label_img[row_0][row_1][row_2][0]  \n",
    "#     def process_label_image(self,image_path):\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df[idx]\n",
    "#         print(idx)\n",
    "        imgs = []\n",
    "        label_imgs = []\n",
    "        list_file = glob.glob(f\"{jpeg_dir}/{row[0]}/{row[1]}/*.jpg\")\n",
    "        list_slice = sorted(list_file, key=lambda x : int(x.split(\"/\")[-1].split(\"_\")[0]))\n",
    "        \n",
    "        randIndex = random.sample(range(len(list_slice)), 60)\n",
    "        randIndex.sort()\n",
    "        list_slice_60 = [list_slice[i] for i in randIndex]\n",
    "        \"\"\"\n",
    "        for i in list_slice_60:\n",
    "#             print(i)\n",
    "            img = cv2.imread(i)\n",
    "            if self.transforms:\n",
    "#                 print(\"tran\")\n",
    "                img = self.transforms(image=img)['image']\n",
    "            if self.preprocessing:\n",
    "                img = self.preprocessing(image=img)['image']\n",
    "            imgs.append(img)\n",
    "            name_split = i.split(\"/\")\n",
    "            row_0 = name_split[-3]\n",
    "            row_1 = name_split[-2]\n",
    "            row_2 = name_split[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "            label_imgs.append(self.df_label_img[row_0][row_1][row_2][0])\n",
    "        \"\"\"\n",
    "        results = self.pool.map(self.process_one_image, list_slice_60)\n",
    "        for img, label in results:\n",
    "            imgs.append(img)\n",
    "            label_imgs.append(label)\n",
    "#         return result\n",
    "\n",
    "        \n",
    "        label_scan = row[4:].astype(int)\n",
    "#         label[2:] = label[2:] if label[0]==1 else 0\n",
    "        neg_exam = label_scan[0]\n",
    "        right_side = label_scan[1]\n",
    "        left_side = label_scan[2]\n",
    "        central = label_scan[3]\n",
    "        gte_1 = label_scan[4]\n",
    "        lt_1 = label_scan[5]\n",
    "        chronic = label_scan[6]\n",
    "        acute_and_chronic = label_scan[7]\n",
    "        acute = 1 - chronic - acute_and_chronic\n",
    "        indeter = label_scan[8]\n",
    "        \n",
    "    \n",
    "        imgs = np.array(imgs)\n",
    "        label_imgs = np.array(label_imgs)\n",
    "        imgs = torch.from_numpy(imgs)\n",
    "        label_imgs = torch.from_numpy(label_imgs)\n",
    "        label_scan = np.array([neg_exam,right_side,left_side,central,gte_1,lt_1,chronic,acute_and_chronic,acute,indeter])\n",
    "        return imgs, label_imgs,torch.from_numpy(label_scan.reshape(-1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def update_train_df(self):\n",
    "        df0 = self.df_main[self.df_main[:,3]==0]\n",
    "        df1 = self.df_main[self.df_main[:,3]==1]\n",
    "        np.random.shuffle(df0)\n",
    "        self.df = np.concatenate([df0[:len(df1)],df1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StudyInstanceUID = list(set(train_df['StudyInstanceUID']))\n",
    "print(len(StudyInstanceUID))\n",
    "t_df = train_df[train_df['StudyInstanceUID'].isin(StudyInstanceUID[0:6500])]\n",
    "v_df = train_df[train_df['StudyInstanceUID'].isin(StudyInstanceUID[6500:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.df_label_img['f03743c2432e']['62a77ef402d1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df[['StudyInstanceUID','SeriesInstanceUID','SOPInstanceUID','pe_present_on_image','negative_exam_for_pe','rightsided_pe','leftsided_pe','central_pe','rv_lv_ratio_gte_1','rv_lv_ratio_lt_1','chronic_pe','acute_and_chronic_pe','indeterminate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df[t_df['StudyInstanceUID']=='f03743c2432e']['pe_present_on_image'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_fn = functools.partial(preprocess_input, **formatted_settings)\n",
    "\n",
    "train_dataset = CTDataset2D(t_df, preprocessing=get_preprocessing(preprocessing_fn), mode='train')\n",
    "\n",
    "\n",
    "# train_dataset = CTDataset2D(t_df,transforms=get_training_augmentation(), preprocessing=get_preprocessing(preprocessing_fn), mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = train_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                             shuffle=True,\n",
    "                             batch_size=4,\n",
    "                             num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet18,resnext50_32x4d\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "class RsnaImageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "#         self.backbone = resnext50_32x4d(pretrained=True)\n",
    "        self.backbone = resnet18 (pretrained=False)\n",
    "        \n",
    "\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            3,\n",
    "            self.backbone.conv1.out_channels,\n",
    "            kernel_size=self.backbone.conv1.kernel_size,\n",
    "            stride=self.backbone.conv1.stride,\n",
    "            padding=self.backbone.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        \n",
    "        # This is 512 for resnet18 and resnet34;\n",
    "        # And it is 2048 for the other resnets\n",
    "        backbone_out_features = 512\n",
    "        # X, Y coords for the future positions (output shape: Bx50x2)\n",
    "        # You can add more layers here.\n",
    "        self.head = nn.Sequential(\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
    "        )\n",
    "        self.logit = nn.Linear(4096, out_features=512)\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        x = self.backbone.conv1(xs[0])\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.head(x)\n",
    "        x = self.logit(x)\n",
    "#         print(x.shape)\n",
    "        out = torch.unsqueeze(x,0)\n",
    "        for i in range(1,len(xs)):\n",
    "            x = self.backbone.conv1(xs[i])\n",
    "            x = self.backbone.bn1(x)\n",
    "            x = self.backbone.relu(x)\n",
    "            x = self.backbone.maxpool(x)\n",
    "\n",
    "            x = self.backbone.layer1(x)\n",
    "            x = self.backbone.layer2(x)\n",
    "            x = self.backbone.layer3(x)\n",
    "            x = self.backbone.layer4(x)\n",
    "            x = self.backbone.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.head(x)\n",
    "            x = self.logit(x)\n",
    "            x = torch.unsqueeze(x,0)\n",
    "            \n",
    "            out = torch.cat((out,x),0)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RsnaLSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_sz  = 512 #(2000 from fcn_en_output reshape to 50*40)\n",
    "        self.hidden_sz = 1024\n",
    "#         self.hidden_sz_en = 128\n",
    "        self.num_layer = 10\n",
    "        \n",
    "        self.interlayer = 512\n",
    "        \n",
    "        self.encoderLSTM = nn.LSTM(self.input_sz,self.hidden_sz,self.num_layer,batch_first=True)\n",
    "#         self.Decoder_lstm = nn.LSTM( self.input_sz,self.hidden_sz,self.num_layer,batch_first=True)\n",
    "\n",
    "        \n",
    "        self.fcn_en_state_dec_state= nn.Sequential(nn.Linear(in_features=self.hidden_sz, out_features=self.interlayer),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Linear(in_features=self.interlayer, out_features=1024)\n",
    "                            )\n",
    "\n",
    "\n",
    "    def forward(self, embeds):\n",
    "        out,(hn,cn) = self.encoderLSTM(embeds)\n",
    "#         print(x.shape)\n",
    "        x = out[:,-1,:]\n",
    "        x = self.fcn_en_state_dec_state(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummaryX import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = RsnaLSTMModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(lstm,(60,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llll=lstm(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = nn.Sequential(nn.Linear(in_features=1024, out_features=2),\n",
    "                                      nn.Softmax(dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xxx(llll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RsnaFullModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "#         self.backbone = resnext50_32x4d(pretrained=True)\n",
    "        self.image_model = RsnaImageModel()\n",
    "        self.lstm_model = RsnaLSTMModel()\n",
    "        self.image_feature = 512\n",
    "        self.study_feature = 1024\n",
    "        self.logit_image = nn.Sequential(nn.Linear(in_features=self.image_feature, out_features=1),\n",
    "                                      nn.Sigmoid())\n",
    "        \n",
    "        self.neg_exam = nn.Sequential(nn.Linear(in_features=self.study_feature, out_features=1),\n",
    "                                      nn.Sigmoid())\n",
    "        self.side_PE = nn.Sequential(nn.Linear(in_features=self.study_feature, out_features=3),\n",
    "                                      nn.Softmax(dim=1))\n",
    "        self.ratio = nn.Sequential(nn.Linear(in_features=self.study_feature, out_features=2),\n",
    "                                      nn.Softmax(dim=1))\n",
    "        self.type_PE = nn.Sequential(nn.Linear(in_features=self.study_feature, out_features=3),\n",
    "                                      nn.Softmax(dim=1))\n",
    "        self.QA = nn.Sequential(nn.Linear(in_features=self.study_feature, out_features=2),\n",
    "                                      nn.Softmax(dim=1))\n",
    "    def forward(self, xs):\n",
    "        img_feature = self.image_model(xs)\n",
    "        study_out = self.lstm_model(img_feature)\n",
    "        img_out = self.logit_image(img_feature)   \n",
    "        \n",
    "        neg_exam_out = self.neg_exam(study_out)\n",
    "        side_PE_out = self.side_PE(study_out)\n",
    "        ratio_out = self.ratio(study_out)\n",
    "        type_PE_out = self.type_PE(study_out)\n",
    "#         QA = self.QA(study_out)\n",
    "        \n",
    "        scan_out = torch.cat([neg_exam_out,side_PE_out,ratio_out,type_PE_out],1)\n",
    "        return img_out,scan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full = RsnaFullModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RsnaImageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model,(60,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = data[0]\n",
    "re = model_full(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_full\n",
    "del re\n",
    "del inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['{}_pe_present_on_image',\n",
    " '{}_negative_exam_for_pe',\n",
    " '{}_qa_motion',\n",
    " '{}_qa_contrast',\n",
    " '{}_flow_artifact',\n",
    " '{}_rv_lv_ratio_gte_1',\n",
    " '{}_rv_lv_ratio_lt_1',\n",
    " '{}_leftsided_pe',\n",
    " '{}_chronic_pe',\n",
    " '{}_true_filling_defect_not_pe',\n",
    " '{}_rightsided_pe',\n",
    " '{}_acute_and_chronic_pe',\n",
    " '{}_central_pe',\n",
    " '{}_indeterminate']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
